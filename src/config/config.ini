[strings]
# Mode : train, test, serve
train_data_file = ./data/train.txt
dev_data_file = ./data/test.txt
test_data_file = ./data/test.txt

# Pretrain model
pretrain_model_path = D:/Spyder/pretrain_model/modelscope/nlp_gpt3_text-generation_chinese-base/
pretrain_model_type = bert-base

# gpu ids
gpu_ids = 1
# save para
output_path = ./output/
log_file = ./log/log.log
model_name = None

[ints]
# model para
max_length = 128
max_gene_length = 128
epochs = 10
batch_size = 32
dev_batch_size = 6
seed = 1234
local_rank = 0
eval_interval = 20
optimizer_step = 64
eval_min_start_epoch = 1
require_improvement = 150
pre_epoch_step_print = 10
gradient_accumulation_steps = 1

[floats]
learning_rate = 3e-5
dropout_rate = 0.2
max_grad_norm = 1.0
warmup_ratio = 0.005
mask_ratio = 0.15
temperature = 0.07
base_temperature = 0.07
add_noise_rate=0.5
beta1 = 0.9
beta2 = 0.98
weight_decay = 0.01

[bools]
mask = True
mlm_loss = True
